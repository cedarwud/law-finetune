from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from peft import PeftModel
import torch

base_model_path = "../LLaMA-Factory/models/DeepSeek-R1-Distill-Qwen-32B"
adapter_path = "../LLaMA-Factory/saves/DeepSeek-R1-Distill-Qwen-32B-QLoRA-4bit-3280-fixed"
merged_model_path = "../LLaMA-Factory/models/DeepSeek-R1-Distill-Qwen-32B-merged"

# ğŸš€ ä½¿ç”¨ 4-bit é‡åŒ–è¨­å®šï¼Œé™ä½ VRAM å ç”¨
bnb_config = BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_compute_dtype=torch.float16,  # è¨ˆç®—æ™‚ä½¿ç”¨ FP16
    bnb_4bit_use_double_quant=True,  # é–‹å•Ÿ double quantï¼Œæ¸›å°‘è¨˜æ†¶é«”ä½¿ç”¨
    bnb_4bit_quant_type="nf4"  # ä½¿ç”¨ NF4 é‡åŒ–æ ¼å¼ï¼Œæé«˜æ•ˆèƒ½
)

# ğŸš€ è¼‰å…¥åŸºç¤æ¨¡å‹ï¼Œå¼·åˆ¶ä½¿ç”¨ `flash_attention_2`
base_model = AutoModelForCausalLM.from_pretrained(
    base_model_path,
    quantization_config=bnb_config,
    attn_implementation="flash_attention_2",  # âœ… å•Ÿç”¨ Flash Attention åŠ é€Ÿæ¨ç†
    device_map="auto"
)

# ğŸš€ è¼‰å…¥ tokenizer
tokenizer = AutoTokenizer.from_pretrained(base_model_path)

# ğŸš€ åŠ è¼‰ QLoRA é©é…å™¨
model = PeftModel.from_pretrained(base_model, adapter_path)

# ğŸš€ åˆä½µ LoRA æ¬Šé‡ä¸¦å„²å­˜
print("é–‹å§‹åˆä½µ LoRA æ¬Šé‡...")
model = model.merge_and_unload()

# ğŸš€ å®‰å…¨å„²å­˜ï¼Œé¿å… rounding error
model.save_pretrained(merged_model_path, safe_serialization=True)
tokenizer.save_pretrained(merged_model_path)

print("âœ… æ¨¡å‹åˆä½µå®Œæˆï¼Œå·²ä¿å­˜è‡³:", merged_model_path)
