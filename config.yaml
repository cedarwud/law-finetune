stage: sft
do_train: True
model_name_or_path: models/deepseek-ai/DeepSeek-R1-Distill-Qwen-32B
preprocessing_num_workers: 16
finetuning_type: lora
quantization_bit: 4
template: qwen
flash_attn: auto
enable_liger_kernel: True
dataset_dir: ./law-finetune
dataset: law
cutoff_len: 1024
learning_rate: 0.00005
num_train_epochs: 3.0
max_samples: 100000
per_device_train_batch_size: 1
gradient_accumulation_steps: 2
lr_scheduler_type: cosine
max_grad_norm: 1.0
logging_steps: 5
save_steps: 100
warmup_steps: 0
report_to: none
output_dir: saves/DeepSeek-R1-Distill-Qwen-32B-QLoRA-4bit-50
bf16: True
plot_loss: True
ddp_timeout: 180000000
optim: adamw_torch
lora_rank: 8
lora_alpha: 16
lora_dropout: 0.1
lora_target: all